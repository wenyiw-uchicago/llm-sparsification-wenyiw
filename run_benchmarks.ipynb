{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'gpt2-xl'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# MLM - DeBerta example\n",
    "model_name = 'microsoft/deberta-v2-xlarge'\n",
    "!python huggingface/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
    "    --model_name_or_path $model_name \\\n",
    "    --tokenizer_name $model_name\\\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --do_eval \\\n",
    "    --output_dir results/$model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GLUE - GPT2 example\n",
    "!python huggingface/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
    "  --model_name_or_path local-models/gpt2-0.0\\\n",
    "  --tokenizer_name local-models/gpt2-tokenizer\\\n",
    "  --task_name mnli \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --output_dir results/gpt2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2022 16:28:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\r\n",
      "11/07/2022 16:28:08 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n",
      "_n_gpu=1,\r\n",
      "adafactor=False,\r\n",
      "adam_beta1=0.9,\r\n",
      "adam_beta2=0.999,\r\n",
      "adam_epsilon=1e-08,\r\n",
      "auto_find_batch_size=False,\r\n",
      "bf16=False,\r\n",
      "bf16_full_eval=False,\r\n",
      "data_seed=None,\r\n",
      "dataloader_drop_last=False,\r\n",
      "dataloader_num_workers=0,\r\n",
      "dataloader_pin_memory=True,\r\n",
      "ddp_bucket_cap_mb=None,\r\n",
      "ddp_find_unused_parameters=None,\r\n",
      "ddp_timeout=1800,\r\n",
      "debug=[],\r\n",
      "deepspeed=None,\r\n",
      "disable_tqdm=False,\r\n",
      "do_eval=True,\r\n",
      "do_predict=False,\r\n",
      "do_train=False,\r\n",
      "eval_accumulation_steps=None,\r\n",
      "eval_delay=0,\r\n",
      "eval_steps=None,\r\n",
      "evaluation_strategy=no,\r\n",
      "fp16=False,\r\n",
      "fp16_backend=auto,\r\n",
      "fp16_full_eval=False,\r\n",
      "fp16_opt_level=O1,\r\n",
      "fsdp=[],\r\n",
      "fsdp_min_num_params=0,\r\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\r\n",
      "full_determinism=False,\r\n",
      "generation_max_length=None,\r\n",
      "generation_num_beams=None,\r\n",
      "gradient_accumulation_steps=1,\r\n",
      "gradient_checkpointing=False,\r\n",
      "greater_is_better=None,\r\n",
      "group_by_length=False,\r\n",
      "half_precision_backend=auto,\r\n",
      "hub_model_id=None,\r\n",
      "hub_private_repo=False,\r\n",
      "hub_strategy=every_save,\r\n",
      "hub_token=<HUB_TOKEN>,\r\n",
      "ignore_data_skip=False,\r\n",
      "include_inputs_for_metrics=False,\r\n",
      "jit_mode_eval=False,\r\n",
      "label_names=None,\r\n",
      "label_smoothing_factor=0.0,\r\n",
      "learning_rate=5e-05,\r\n",
      "length_column_name=length,\r\n",
      "load_best_model_at_end=False,\r\n",
      "local_rank=-1,\r\n",
      "log_level=passive,\r\n",
      "log_level_replica=passive,\r\n",
      "log_on_each_node=True,\r\n",
      "logging_dir=resutls/translation/runs/Nov07_16-28-08_wenyi-ubuntu,\r\n",
      "logging_first_step=False,\r\n",
      "logging_nan_inf_filter=True,\r\n",
      "logging_steps=500,\r\n",
      "logging_strategy=steps,\r\n",
      "lr_scheduler_type=linear,\r\n",
      "max_grad_norm=1.0,\r\n",
      "max_steps=-1,\r\n",
      "metric_for_best_model=None,\r\n",
      "mp_parameters=,\r\n",
      "no_cuda=False,\r\n",
      "num_train_epochs=3.0,\r\n",
      "optim=adamw_hf,\r\n",
      "output_dir=resutls/translation,\r\n",
      "overwrite_output_dir=True,\r\n",
      "past_index=-1,\r\n",
      "per_device_eval_batch_size=4,\r\n",
      "per_device_train_batch_size=4,\r\n",
      "predict_with_generate=True,\r\n",
      "prediction_loss_only=False,\r\n",
      "push_to_hub=False,\r\n",
      "push_to_hub_model_id=None,\r\n",
      "push_to_hub_organization=None,\r\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\n",
      "ray_scope=last,\r\n",
      "remove_unused_columns=True,\r\n",
      "report_to=['tensorboard'],\r\n",
      "resume_from_checkpoint=None,\r\n",
      "run_name=resutls/translation,\r\n",
      "save_on_each_node=False,\r\n",
      "save_steps=500,\r\n",
      "save_strategy=steps,\r\n",
      "save_total_limit=None,\r\n",
      "seed=42,\r\n",
      "sharded_ddp=[],\r\n",
      "skip_memory_metrics=True,\r\n",
      "sortish_sampler=False,\r\n",
      "tf32=None,\r\n",
      "torchdynamo=None,\r\n",
      "tpu_metrics_debug=False,\r\n",
      "tpu_num_cores=None,\r\n",
      "use_ipex=False,\r\n",
      "use_legacy_prediction_loop=False,\r\n",
      "use_mps_device=False,\r\n",
      "warmup_ratio=0.0,\r\n",
      "warmup_steps=0,\r\n",
      "weight_decay=0.0,\r\n",
      "xpu_backend=None,\r\n",
      ")\r\n",
      "11/07/2022 16:28:09 - INFO - datasets.info - Loading Dataset Infos from /home/wenyi/.cache/huggingface/modules/datasets_modules/datasets/wmt16/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\r\n",
      "11/07/2022 16:28:09 - INFO - datasets.builder - Overwrite dataset info from restored data version.\r\n",
      "11/07/2022 16:28:09 - INFO - datasets.info - Loading Dataset info from /home/wenyi/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\r\n",
      "11/07/2022 16:28:09 - WARNING - datasets.builder - Found cached dataset wmt16 (/home/wenyi/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\r\n",
      "11/07/2022 16:28:09 - INFO - datasets.info - Loading Dataset info from /home/wenyi/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227\r\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1690.80it/s]\r\n",
      "[INFO|configuration_utils.py:654] 2022-11-07 16:28:09,332 >> loading configuration file config.json from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\r\n",
      "[INFO|configuration_utils.py:706] 2022-11-07 16:28:09,341 >> Model config T5Config {\r\n",
      "  \"_name_or_path\": \"t5-small\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5WithLMHeadModel\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 512,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dense_act_fn\": \"relu\",\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"relu\",\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"is_gated_act\": false,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"n_positions\": 512,\r\n",
      "  \"num_decoder_layers\": 6,\r\n",
      "  \"num_heads\": 8,\r\n",
      "  \"num_layers\": 6,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_max_distance\": 128,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 200,\r\n",
      "      \"min_length\": 30,\r\n",
      "      \"no_repeat_ngram_size\": 3,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"summarize: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_de\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to German: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_fr\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to French: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_ro\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to Romanian: \"\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32128\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_auto.py:427] 2022-11-07 16:28:09,485 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\r\n",
      "[INFO|configuration_utils.py:654] 2022-11-07 16:28:09,620 >> loading configuration file config.json from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\r\n",
      "[INFO|configuration_utils.py:706] 2022-11-07 16:28:09,623 >> Model config T5Config {\r\n",
      "  \"_name_or_path\": \"t5-small\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5WithLMHeadModel\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 512,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dense_act_fn\": \"relu\",\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"relu\",\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"is_gated_act\": false,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"n_positions\": 512,\r\n",
      "  \"num_decoder_layers\": 6,\r\n",
      "  \"num_heads\": 8,\r\n",
      "  \"num_layers\": 6,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_max_distance\": 128,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 200,\r\n",
      "      \"min_length\": 30,\r\n",
      "      \"no_repeat_ngram_size\": 3,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"summarize: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_de\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to German: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_fr\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to French: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_ro\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to Romanian: \"\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32128\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-07 16:28:09,905 >> loading file spiece.model from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/spiece.model\r\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-07 16:28:09,905 >> loading file tokenizer.json from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-07 16:28:09,905 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-07 16:28:09,905 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-07 16:28:09,906 >> loading file tokenizer_config.json from cache at None\r\n",
      "[INFO|configuration_utils.py:654] 2022-11-07 16:28:09,906 >> loading configuration file config.json from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\r\n",
      "[INFO|configuration_utils.py:706] 2022-11-07 16:28:09,908 >> Model config T5Config {\r\n",
      "  \"_name_or_path\": \"t5-small\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"T5WithLMHeadModel\"\r\n",
      "  ],\r\n",
      "  \"d_ff\": 2048,\r\n",
      "  \"d_kv\": 64,\r\n",
      "  \"d_model\": 512,\r\n",
      "  \"decoder_start_token_id\": 0,\r\n",
      "  \"dense_act_fn\": \"relu\",\r\n",
      "  \"dropout_rate\": 0.1,\r\n",
      "  \"eos_token_id\": 1,\r\n",
      "  \"feed_forward_proj\": \"relu\",\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"is_gated_act\": false,\r\n",
      "  \"layer_norm_epsilon\": 1e-06,\r\n",
      "  \"model_type\": \"t5\",\r\n",
      "  \"n_positions\": 512,\r\n",
      "  \"num_decoder_layers\": 6,\r\n",
      "  \"num_heads\": 8,\r\n",
      "  \"num_layers\": 6,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"relative_attention_max_distance\": 128,\r\n",
      "  \"relative_attention_num_buckets\": 32,\r\n",
      "  \"task_specific_params\": {\r\n",
      "    \"summarization\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"length_penalty\": 2.0,\r\n",
      "      \"max_length\": 200,\r\n",
      "      \"min_length\": 30,\r\n",
      "      \"no_repeat_ngram_size\": 3,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"summarize: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_de\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to German: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_fr\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to French: \"\r\n",
      "    },\r\n",
      "    \"translation_en_to_ro\": {\r\n",
      "      \"early_stopping\": true,\r\n",
      "      \"max_length\": 300,\r\n",
      "      \"num_beams\": 4,\r\n",
      "      \"prefix\": \"translate English to Romanian: \"\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 32128\r\n",
      "}\r\n",
      "\r\n",
      "/home/wenyi/anaconda3/envs/sparsification/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\r\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\r\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\r\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\r\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|modeling_utils.py:2158] 2022-11-07 16:28:09,954 >> loading weights file pytorch_model.bin from cache at /home/wenyi/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:2609] 2022-11-07 16:28:10,384 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:2617] 2022-11-07 16:28:10,384 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\r\n",
      "Running tokenizer on validation dataset:   0%|            | 0/2 [00:00<?, ?ba/s]11/07/2022 16:28:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/wenyi/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/cache-27fc2559381e0bf2.arrow\r\n",
      "Running tokenizer on validation dataset:  50%|██  | 1/2 [00:00<00:00, 14.21ba/s]\r\n",
      "11/07/2022 16:28:11 - INFO - __main__ - *** Evaluate ***\r\n",
      "[INFO|trainer.py:2943] 2022-11-07 16:28:11,468 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:2945] 2022-11-07 16:28:11,468 >>   Num examples = 1999\r\n",
      "[INFO|trainer.py:2948] 2022-11-07 16:28:11,468 >>   Batch size = 4\r\n",
      "[WARNING|logging.py:281] 2022-11-07 16:28:11,469 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "100%|█████████████████████████████████████████| 500/500 [02:07<00:00,  3.92it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  eval_bleu               =    26.1606\r\n",
      "  eval_gen_len            =    42.1931\r\n",
      "  eval_loss               =      1.419\r\n",
      "  eval_runtime            = 0:02:08.23\r\n",
      "  eval_samples            =       1999\r\n",
      "  eval_samples_per_second =     15.589\r\n",
      "  eval_steps_per_second   =      3.899\r\n",
      "[INFO|modelcard.py:449] 2022-11-07 16:30:19,854 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 ro-en', 'type': 'wmt16', 'args': 'ro-en'}}\r\n"
     ]
    }
   ],
   "source": [
    "# T-5 translation example\n",
    "!python huggingface/transformers/examples/pytorch/translation/run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_eval \\\n",
    "    --source_lang en \\\n",
    "    --target_lang ro \\\n",
    "    --source_prefix \"translate English to Romanian: \" \\\n",
    "    --dataset_name wmt16 \\\n",
    "    --dataset_config_name ro-en \\\n",
    "    --output_dir resutls/translation \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
